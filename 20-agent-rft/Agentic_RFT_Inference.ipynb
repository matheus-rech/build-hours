{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "from utils_shared.tracing_storing import run_tool_eval\n",
    "from utils_shared.tracing_storing import EvalParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"finqa_model_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_module = import_module(f\"build_hour.{project}.utils_tools.openai_client\")\n",
    "client = client_module.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets used for training\n",
    "train_file = f\"build_hour/{project}/data/{project}_train.jsonl\"\n",
    "test_file = f\"build_hour/{project}/data/{project}_val.jsonl\"\n",
    "\n",
    "with open(train_file, \"r\") as f:\n",
    "    train_items = [json.loads(line) for line in f]\n",
    "\n",
    "with open(test_file, \"r\") as f:\n",
    "    test_items = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text format used for training\n",
    "text_format_module = import_module(f\"build_hour.{project}.utils_tools.text_format\")\n",
    "RESPONSE_FORMAT_RESPONSES = text_format_module.RESPONSE_FORMAT_RESPONSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the graders used for training\n",
    "graders_module = import_module(f\"build_hour.{project}.utils_tools.graders\")\n",
    "GRADERS = graders_module.GRADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tools used for training\n",
    "tools_module = import_module(f\"build_hour.{project}.utils_tools.tools\")\n",
    "TOOL_NAME_TO_FUNC = tools_module.TOOL_NAME_TO_FUNC\n",
    "TOOLS_RESPONSES = tools_module.TOOLS_RESPONSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS_RESPONSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-5\"\n",
    "\n",
    "reasoning_effort = \"medium\"\n",
    "run_name = f\"{model}-{reasoning_effort}\"\n",
    "\n",
    "eval_params = EvalParams(\n",
    "    project=project,\n",
    "    run_name=run_name,\n",
    "    model=model,\n",
    "    reasoning_effort=reasoning_effort,\n",
    "    graders=GRADERS,\n",
    "    text={\"format\": RESPONSE_FORMAT_RESPONSES},\n",
    "    tools=TOOLS_RESPONSES,\n",
    "    tool_name_to_func=TOOL_NAME_TO_FUNC,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_items), len(test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    results, output_jsonl = run_tool_eval(\n",
    "        items=test_items[:5],\n",
    "        eval_params=eval_params,\n",
    "        client=client,\n",
    "        verbose=False,\n",
    "        max_workers=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all .jsonl files under tool_evals recursively\n",
    "jsonl_files = list(\n",
    "    Path(\n",
    "        f\"build_hour/{eval_params.project}/tool_evals/{eval_params.run_name}/\" # you can add the evalrun_id here if you want to access a specific run's results\n",
    "    ).rglob(\"results.jsonl\")\n",
    ")\n",
    "print(jsonl_files)\n",
    "\n",
    "# For each grader, collect a list of per-run means\n",
    "grader_run_means = {grader_name: [] for grader_name in GRADERS.keys()}\n",
    "grader_all_scores = {grader_name: [] for grader_name in GRADERS.keys()}  # Collect all scores for each grader\n",
    "run_durations = []\n",
    "all_durations = []  # Collect all durations across all runs\n",
    "\n",
    "for i, jsonl_path in enumerate(jsonl_files):\n",
    "    print(f\"Processing {i+1}/{len(jsonl_files)}\")\n",
    "    run_scores = {grader_name: [] for grader_name in GRADERS.keys()}\n",
    "    run_durs = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            metrics = obj.get(\"metrics\", {})\n",
    "            for grader in run_scores.keys():\n",
    "                value = metrics.get(grader)\n",
    "                if isinstance(value, float):\n",
    "                    run_scores[grader].append(value)\n",
    "                    grader_all_scores[grader].append(value)\n",
    "                elif isinstance(value, dict):\n",
    "                    reward = value.get(\"reward\")\n",
    "                    if isinstance(reward, float):\n",
    "                        run_scores[grader].append(reward)\n",
    "                        grader_all_scores[grader].append(reward)\n",
    "            duration = obj.get(\"duration_seconds\")\n",
    "            if duration is not None:\n",
    "                run_durs.append(duration)\n",
    "                all_durations.append(duration)  # Add to the global list\n",
    "    # Compute mean for each grader in this run and store\n",
    "    for grader, values in run_scores.items():\n",
    "        if len(values) > 0:\n",
    "            grader_run_means[grader].append(np.mean(values))\n",
    "    # Compute mean duration for this run\n",
    "    if len(run_durs) > 0:\n",
    "        run_durations.append(np.mean(run_durs))\n",
    "\n",
    "# Now compute mean and std of the per-run means for each grader\n",
    "for grader, means in grader_run_means.items():\n",
    "    arr = np.array(means)\n",
    "    print(f\"{grader} (mean of run means): {np.mean(arr):.3f} ± {np.std(arr):.3f}\")\n",
    "\n",
    "arr_durations = np.array(run_durations)\n",
    "print(f\"Duration (seconds, mean of run means): {np.mean(arr_durations):.2f} ± {np.std(arr_durations):.2f}\")\n",
    "\n",
    "# Also provide the array of all durations\n",
    "all_durations = np.array(all_durations)\n",
    "print(f\"All durations array shape: {all_durations.shape}\")\n",
    "\n",
    "# Also provide the arrays of all scores for each grader\n",
    "for grader, scores in grader_all_scores.items():\n",
    "    grader_all_scores[grader] = np.array(scores)\n",
    "    print(f\"All scores for {grader}: shape {grader_all_scores[grader].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Define 30 bins between 0 and 1 (31 edges)\n",
    "bin_edges = np.linspace(0, 1, 31)\n",
    "\n",
    "fig = px.histogram(\n",
    "    grader_all_scores[grader],\n",
    "    x=grader_all_scores[grader],\n",
    "    title=f\"Distribution of scores for {grader}\",\n",
    "    labels={'x': 'Score', 'y': 'Frequency'},\n",
    "    nbins=None\n",
    ")\n",
    "fig.update_traces(xbins=dict(\n",
    "    start=bin_edges[0],\n",
    "    end=bin_edges[-1],\n",
    "    size=(bin_edges[1] - bin_edges[0])\n",
    "))\n",
    "fig.update_layout(\n",
    "    bargap=0.1, \n",
    "    template=\"plotly_white\",\n",
    "    font=dict(family=\"Helvetica Neue, Helvetica, Arial, sans-serif\", size=16),\n",
    "    showlegend=False,\n",
    "    xaxis=dict(range=[0, 1])\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(\n",
    "    all_durations,\n",
    "    nbins=30,\n",
    "    title=\"Distribution of durations\",\n",
    "    labels={'value': 'Duration (s)', 'count': 'Frequency'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    bargap=0.1,\n",
    "    template=\"plotly_white\",\n",
    "    font=dict(family=\"Helvetica Neue, Helvetica, Arial, sans-serif\", size=16),\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "og_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
